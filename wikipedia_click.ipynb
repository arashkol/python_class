{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OUVOCRcQXkm0w87WCCPEUFC2JXDZsPeC",
      "authorship_tag": "ABX9TyPk1mzFo3Ack2D92n+bYPPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashkol/python_class/blob/main/wikipedia_click.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wikipedia artcile recommender\n",
        "##This program receives a keyword and recommends Wikipedia artciles in order to better understand the keyword. This recommender system uses click behavior of the users and the search to find related artciles.\n",
        "\n",
        "The idea is that if the users have frequently clicked on a link in a wikipedia pages, while reading that page, the target/clicked page is a prerequisite for the current page. In case no click data is available, relevant artciles to the keyword as a result of the Wikipedia fuzzy search will be recommended to the user.\n",
        "\n",
        "In this program Wikipedia API are used for search and retrival of information. The monthly published click stream of Wikipedia is also used as a guide for the recommender. The clisk stream could be downlowded here:\n",
        "\n",
        "\n",
        "https://dumps.wikimedia.org/other/clickstream"
      ],
      "metadata": {
        "id": "BAhkAQw7yg1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Technologies used:\n",
        "[for loop](#for_loop)\n",
        "\n",
        "[while loop](#while_loop)\n",
        "\n",
        "[class](#class)\n",
        "\n",
        "[class inheritance](#class_inheritance)\n",
        "\n",
        "[function](#function)\n",
        "\n",
        "[recursive function](#recursive_function)\n"
      ],
      "metadata": {
        "id": "X23d1EmrJ8dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the wikipedia module for using Wikipedia API\n",
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "8O9JbA__p7Iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c98b67f-6bab-4093-f3ee-3e9d7384ae6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (4.0.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=3a4b23f6ce5d142574f2eb6fdaac76afdeb8170619c12e334be1798e9c542513\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/93/05/72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing libraries\n",
        "##main libraries used: \n",
        "###**Pandas** for data processing\n",
        "###**Wikipedia** for retrieving information about Wikipedia articles using Wikipedia API"
      ],
      "metadata": {
        "id": "FUspPTEPmdd5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AKFbadrMPgbA"
      },
      "outputs": [],
      "source": [
        "from curses.ascii import isascii #recognises if the letters a latin and numerical\n",
        "import requests #used for web interaction\n",
        "import gzip #used for unzipping the downloaded wikipedia click stream\n",
        "import re #reqular expressions for string processing\n",
        "from urllib.parse import quote #for replacing special charachters inside URLs with %xx escape.\n",
        "import pandas as pd #used for tabular data anaylisis \n",
        "import wikipedia # Wikipedia API module\n",
        "import random as rnd #for random integer generation\n",
        "import numpy as np #for arithmatic operations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Caiustion: if the data is already preprocessed and saved, ignore the following rows and start from: [Load preprocced data.](#cell-id)\n"
      ],
      "metadata": {
        "id": "ky0qAiR8DVpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Retrieving data from the Wikipedia click-stream:\n",
        "###Here the click data of several months is retrieved and concatinated.\n",
        "\n",
        "Techniques used:\n",
        "**for-loop**"
      ],
      "metadata": {
        "id": "vEI4aAyvnA3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of month names to retrieve\n",
        "#One month of clicks may not cover all keywords we need\n",
        "months = [\"2021-07\", \"2021-08\", \"2021-09\", \"2021-10\", \"2021-11\", \"2021-12\"]\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "all_data = []\n",
        "\n",
        "# Loop through each month and retrieve the data\n",
        "for month in months:\n",
        "    url = f\"https://dumps.wikimedia.org/other/clickstream/{month}/clickstream-zhwiki-{month}.tsv.gz\"\n",
        "    response = requests.get(url)\n",
        "    data = gzip.decompress(response.content)\n",
        "    \n",
        "    # Append the data to the running list\n",
        "    all_data.append(data)\n",
        "\n",
        "# Concatenate all data together into a single string\n",
        "all_data = b\"\".join(all_data)"
      ],
      "metadata": {
        "id": "YZjQ05nnMV9e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data cleaning and preparation"
      ],
      "metadata": {
        "id": "CAkAbDMenla2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert bytes to unicode \n",
        "data = all_data.decode(\"utf-8\")\n",
        "#we have to do this because the all_data typy is byte encoded to \"utf-8\""
      ],
      "metadata": {
        "id": "d8Js9KowP9pP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#An exampe of how utf-8 encoding works:\n",
        "array = \"München Ü\"\n",
        "byte_array = array.encode(\"utf-8\")\n",
        "char_to_find = \"ü\".encode(\"utf-8\")  # encoded representation of \"ü\" in utf-8\n",
        "index_of_char = byte_array.index(char_to_find)\n",
        "print(array)\n",
        "print(char_to_find)\n",
        "print(byte_array)\n",
        "print(index_of_char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnRLufkv5QSk",
        "outputId": "54618a03-c097-4802-d348-6edcde6e3ea4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "München Ü\n",
            "b'\\xc3\\xbc'\n",
            "b'M\\xc3\\xbcnchen \\xc3\\x9c'\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the  string contains illegal characters: -~ -ÿĀ-῿Ⰰ-퟿豈-﷏ﷰ-�\n",
        "def good_string(string_):\n",
        "    if re.search(r\"[^\\u0020-\\u007E\\u00A0-\\u00FF\\u0100-\\u1FFF\\u2C00-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD]\", string_):\n",
        "        return False\n",
        "\n",
        "    if (string_ == None): return False\n",
        "    # Check if the string is properly encodable in UTF-8\n",
        "    try:\n",
        "        string_.encode(\"utf-8\")\n",
        "    except UnicodeEncodeError:\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "OEpVFE8vRQmM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if the string is actually a number\n",
        "def is_number(string_):\n",
        "    try:\n",
        "        float(string_)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "7tGTyNmIRTYV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#does the string contain just ASCII charachters? For example, Ü,Ö,Ä are not ASCII\n",
        "def has_only_ascii(string_):\n",
        "    return all(char.isascii() for char in string_)"
      ],
      "metadata": {
        "id": "o4xShLm3RVrM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of ASCII vs non-ASCII characters recognized inside strings by has_only_ascii() function\n",
        "print(has_only_ascii(\"Munich\"), has_only_ascii(\"München\"), has_only_ascii(\"Мюншен\"), has_only_ascii(\"مونیخ\")  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHo8GyFgBs2x",
        "outputId": "0eea8826-548d-4243-9d4d-76b52e8fe98d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True False False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define an empty dataset/DataFrame\n",
        "df_clicks = pd.DataFrame(columns=['from','clicks','to']) "
      ],
      "metadata": {
        "id": "VW4Yc10WRaNC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing\n",
        "<a name=\"for_loop\"></a>"
      ],
      "metadata": {
        "id": "BIAY7Qld0hKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for line in data.split(\"\\n\"):\n",
        "\n",
        "    columns = line.split(\"\\t\")\n",
        "\n",
        "    # Skip the header row and the row containing non-keyword strings\n",
        "    if columns[0] == \"source\" or len(columns)<2 \\\n",
        "    or columns[0] == \"other-empty\" \\\n",
        "    or \"other-search\" in columns[0] or \"other-internal\" in columns[0] or \"other-external\" in columns[0]\\\n",
        "    or \"other-other\" in columns[0] or \"other-other\" in columns[2]\\\n",
        "    or \"other-empty\" in columns[1]\\\n",
        "    or \"other-search\" in columns[1] or \"other-internal\" in columns[1] or \"other-external\" in columns[1]\\\n",
        "    or \"other-empty\" in columns[2]\\\n",
        "    or \"other-search\" in columns[2] or  \"other-internal\" in columns[2] or \"other-external\" in columns[2]\\\n",
        "    or not is_number(columns[3]) or not has_only_ascii(columns[0])or not has_only_ascii(columns[1]):\n",
        "        continue\n",
        "\n",
        "    #replace special characters in URLs with xx% charachters e.g.: \"+\" -> \"%2B\"   \n",
        "    source = quote(columns[0]) #from\n",
        "    target = quote(columns[1]) #to\n",
        "    clicks = columns[3] #number of clicks\n",
        "\n",
        "    #if the everyithing is ok; not bad charachter is in the strings, make a \n",
        "    #dictionary out of them and add the to them and add to the DataFrame\n",
        "    if good_string(source) and good_string(target):\n",
        "        df_clicks = df_clicks.append({\"from\":source, \"clicks\":clicks, \"to\":target},ignore_index=True)"
      ],
      "metadata": {
        "id": "PZNU5s4hQFM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How many rows/recods do we have?"
      ],
      "metadata": {
        "id": "9vbYWS2t6Da_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of records:\",len(df_clicks))"
      ],
      "metadata": {
        "id": "K6eITOIj6I8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Replacing non-informative strings with null and then removing nulles"
      ],
      "metadata": {
        "id": "yz9VVZ5w57W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove all rows containing empty strings of null values\n",
        "df_clicks = df_clicks.replace(\"\", np.nan)\n",
        "df_clicks.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "lUp1NddpT7at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How many recodes remained after removing null values?"
      ],
      "metadata": {
        "id": "E4fwNlX-6Ur2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#see how many rows remained:\n",
        "print(\"number of records:\",len(df_clicks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IL13UZ9FwKF",
        "outputId": "c0e7e067-dc64-4573-b657-a4ac81a6a704"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of records: 17834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge the data of all months"
      ],
      "metadata": {
        "id": "EkOKJmnFAjqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#summ all clicks with the identical \"from\" and \"to\" fileds. These identical rows \n",
        "#come from different months but the same source and target pages.\n",
        "df_clicks = df_clicks.groupby([\"from\",\"to\"]).sum() \n",
        "df_clicks.reset_index(inplace = True) #reorder the indecies after applying sum\n",
        "df_clicks = df_clicks[[\"from\",\"clicks\",\"to\"]] #reorder the columns"
      ],
      "metadata": {
        "id": "vJGxseIb0MEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How many recodes remained after merging months?"
      ],
      "metadata": {
        "id": "4Kuxmrr5A1EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#see how many rows remained:\n",
        "print(\"number of records:\",len(df_clicks))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt4lAZiNA5qw",
        "outputId": "94b5cb5f-ef80-4025-deea-60585178507b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of records: 17834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving the results not to always need loading and preprocessing the data:"
      ],
      "metadata": {
        "id": "QQrH4hx3BOZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the data to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df_clicks.to_csv('/content/drive/wikipedia_clicks.csv')"
      ],
      "metadata": {
        "id": "loVSa-bx_Moq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-id\"></a>\n",
        "##If the data is already preprocessed and saved on Google Drive, just load it from the CSV file:"
      ],
      "metadata": {
        "id": "SKHvMew_C_aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clicks.from_csv('/content/drive/wikipedia_clicks.csv')"
      ],
      "metadata": {
        "id": "6JxrUgOqDL8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Definition of child of a page: \n",
        "A page page_child is the child of the page_parent, if in the df_clicks dataset, there is any row in which page_parent is in the \"from\" and the page_child is in the \"to\" field."
      ],
      "metadata": {
        "id": "wbNfgNCtcJdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Rerieve page: <a name=\"function\"></a>"
      ],
      "metadata": {
        "id": "LtpqcAfRL2p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from wikipedia.wikipedia import random\n",
        "#Get the page related to the keyword/query\n",
        "def get_page(query): \n",
        "  #Search for matching pages to the query/keyword\n",
        "  results = wikipedia.search(query)\n",
        " \n",
        "  if len(results) == 0:#if no page is found, as Wikipedia to suggest a page\n",
        "    suggestion = wikipedia.suggest(query)\n",
        "    return wikipedia.page(suggestion) #return the suggested page\n",
        "\n",
        "  if len(results) == 1:#if just one keword is found\n",
        "    try:#try to find the related page and return it\n",
        "      return wikipedia.page(results)\n",
        "    except:#if actually no related page is there on Wikipedia, return Null\n",
        "      return None\n",
        "\n",
        "  #we are going to find the page with the maximum number of children pages\n",
        "  max_child = -1 #initial value for the maximum number of pages\n",
        "  max_page = None\n",
        "\n",
        "  for i in range(len(results)): #for the number of found keywords,\n",
        "    #take keywords one by one\n",
        "    res = results[i] \n",
        "    #how many children does the page have?/how many times people have gone from this page to anothe one?\n",
        "    n_child = len(df_clicks[df_clicks[\"from\"].str.contains(query,case=False)])\n",
        "    if n_child > max_child: #if the number of the children is bigger than the number of the current maximum number of children\n",
        "      max_child = n_child #set the new number of chilred as the maximum and \n",
        "      try: #try to find the page \n",
        "        max_page = wikipedia.page(res) \n",
        "      except:\n",
        "        pass\n",
        "\n",
        "  return max_page"
      ],
      "metadata": {
        "id": "QCfYRfsCpa49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Class/Type/Abstraction/Defenition of an Article:\n",
        "###Each articla is packed inside a class\n",
        "<a name=\"class\"></a>"
      ],
      "metadata": {
        "id": "RxIhRJr169K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#each artice and it's parameters is considered as an objet of the type class \"article\"\n",
        "class article:\n",
        "  #constructor function; initializes all attributes other than the ones which require\n",
        "  #web requests using Wikipedia API.\n",
        "  def __init__(self,keyword, parent = [], children = []):\n",
        "    self.keyword = keyword #wikipedia keyword\n",
        "    self.parent = parent #from which page we people come to the page having the \"keyword\"?\n",
        "    self.children = children #to which page people gone from the page having the \"keyword\"?\n",
        "    self.fill_article_info() #find URL and abstract of the page containing the \"keyword\"\n",
        "  \n",
        "  def fill_article_info(self):    \n",
        "    #retrieves page title and URL and initializes the respective attributes\n",
        "    page_title = get_page(self.keyword) #find the page which moslty matched the keyword\n",
        "    self.page = page_title #get the title of the page\n",
        "    self.article_url = self.page.url #get the urls of the page\n",
        "    print(self.article_url) #print out the URL \n",
        "    print(self.page.summary) #print out the summary\n",
        "    "
      ],
      "metadata": {
        "id": "l7Suw_UShGfW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Article Tree Class\n",
        "<a name=\"class_inheritance\"></a>\n",
        "<a name=\"while_loop\"></a>\n",
        "<a name=\"recursive_function\"></a>"
      ],
      "metadata": {
        "id": "r6XHhBiYMy55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###For each search with specific depth and width, a tree data structure made of article classes is built. Instead of calling a search function recursively and saving the resulting keywords and URLs in separate lists, each found article is packed in an article class. Consecutive linked articles based on the click or search data are stored in a tree class which also preserves the relations among articles."
      ],
      "metadata": {
        "id": "775MHOWUkzEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class article_tree:\n",
        "\n",
        "  def __init__(self, root_keyword, n_depth, n_branches, all_children=None):\n",
        "    self.root_keyword = root_keyword\n",
        "    self.n_depth = n_depth\n",
        "    self.n_branches = n_branches\n",
        "    self.all_children = {root_keyword}#all_children or set()\n",
        "    self.build_tree()\n",
        "\n",
        "\n",
        "  def get_children(self, keyword,n_branches):\n",
        "\n",
        "    children = df_clicks[df_clicks[\"from\"].str.contains(keyword,case=False)].sort_values(\"clicks\",ascending=False).iloc[0:n_branches].to.values\n",
        "    children = list(children)\n",
        "\n",
        "    n_child = len(children)\n",
        "\n",
        "    for ch in self.all_children:\n",
        "      self.all_children.add(ch)\n",
        "\n",
        "    if n_child < n_branches :\n",
        "      results = wikipedia.search(keyword)\n",
        "      i = n_child\n",
        "      k = 0\n",
        "      while i < n_branches and k<len(results):\n",
        "        if results[k] not in self.all_children:        \n",
        "          self.all_children.add(results[k])\n",
        "          children.append(results[k])\n",
        "          i += 1\n",
        "        k += 1\n",
        "    return children\n",
        "\n",
        "  def build_children(self, root_keyword, n_depth, n_branches ): \n",
        "    \n",
        "    root = article(root_keyword)\n",
        "    if n_depth == 1:\n",
        "      return root\n",
        "    \n",
        "    children_names = self.get_children(root_keyword,n_branches)\n",
        "    \n",
        "    children = []\n",
        "    for child_name in children_names:\n",
        "      \n",
        "      art_ = self.build_children(child_name, n_depth-1, n_branches)\n",
        "\n",
        "      children.append(art_)\n",
        "    root.children = children\n",
        "    \n",
        "    return root\n",
        "\n",
        "  def build_tree(self):  \n",
        "    self.root = self.build_children(self.root_keyword, self.n_depth, self.n_branches)"
      ],
      "metadata": {
        "id": "amwnK3XKlhed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcXOpTTf8yVn",
        "outputId": "9cd5b4b8-47c7-417c-a419-f92ab754f5bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = \"chat gpt\"\n",
        "tree_1 = article_tree(keyword,3,3)"
      ],
      "metadata": {
        "id": "OdcsKkbyGiJU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}