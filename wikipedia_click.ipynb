{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashkol/python_class/blob/main/wikipedia_click.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAhkAQw7yg1K"
      },
      "source": [
        "#Wikipedia artcile recommender\n",
        "##This program receives a keyword and recommends Wikipedia artciles in order to better understand the keyword. This recommender system uses click behavior of the users and the search to find related artciles.\n",
        "\n",
        "The idea is that if the users have frequently clicked on a link in a wikipedia pages, while reading that page, the target/clicked page is a prerequisite for the current page. In case no click data is available, relevant artciles to the keyword as a result of the Wikipedia fuzzy search will be recommended to the user.\n",
        "\n",
        "In this program Wikipedia API are used for search and retrival of information. The monthly published click stream of Wikipedia is also used as a guide for the recommender. The clisk stream could be downlowded here:\n",
        "\n",
        "\n",
        "https://dumps.wikimedia.org/other/clickstream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X23d1EmrJ8dH"
      },
      "source": [
        "##Technologies used:\n",
        "[for loop](#for_loop)\n",
        "\n",
        "[while loop](#while_loop)\n",
        "\n",
        "[class](#class)\n",
        "\n",
        "[class inheritance](#class_inheritance)\n",
        "\n",
        "[function](#function)\n",
        "\n",
        "[recursive function](#recursive_function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O9JbA__p7Iu",
        "outputId": "805acd18-31ea-4b3f-a105-6f26d5770bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.8/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.14)\n"
          ]
        }
      ],
      "source": [
        "#Installing the wikipedia module for using Wikipedia API\n",
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUspPTEPmdd5"
      },
      "source": [
        "#Importing libraries\n",
        "##main libraries used: \n",
        "###**Pandas** for data processing\n",
        "###**Wikipedia** for retrieving information about Wikipedia articles using Wikipedia API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKFbadrMPgbA"
      },
      "outputs": [],
      "source": [
        "from curses.ascii import isascii #recognises if the letters a latin and numerical\n",
        "import requests #used for web interaction\n",
        "import gzip #used for unzipping the downloaded wikipedia click stream\n",
        "import re #reqular expressions for string processing\n",
        "from urllib.parse import quote #for replacing special charachters inside URLs with %xx escape.\n",
        "import pandas as pd #used for tabular data anaylisis \n",
        "import wikipedia # Wikipedia API module\n",
        "import random as rnd #for random integer generation\n",
        "import numpy as np #for arithmatic operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky0qAiR8DVpD"
      },
      "source": [
        "##Caution: if the data is already preprocessed and saved, ignore the following rows and start from: [Load preprocced data.](#cell-id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEI4aAyvnA3v"
      },
      "source": [
        "##Retrieving data from the Wikipedia click-stream:\n",
        "###Here the click data of several months is retrieved and concatinated.\n",
        "\n",
        "Techniques used:\n",
        "**for-loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZjQ05nnMV9e"
      },
      "outputs": [],
      "source": [
        "# Define a list of month names to retrieve\n",
        "#One month of clicks may not cover all keywords we need\n",
        "\n",
        "#months = [\"2021-12\"] #short and fast\n",
        "months = [\"2021-07\", \"2021-08\", \"2021-09\", \"2021-10\", \"2021-11\", \"2021-12\"] #long and slow\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "all_data = []\n",
        "\n",
        "# Loop through each month and retrieve the data\n",
        "for month in months:\n",
        "    url = f\"https://dumps.wikimedia.org/other/clickstream/{month}/clickstream-zhwiki-{month}.tsv.gz\"\n",
        "    response = requests.get(url)\n",
        "    data = gzip.decompress(response.content)\n",
        "    \n",
        "    # Append the data to the running list\n",
        "    all_data.append(data)\n",
        "\n",
        "# Concatenate all data together into a single string\n",
        "all_data = b\"\".join(all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAkAbDMenla2"
      },
      "source": [
        "##Data cleaning and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8Js9KowP9pP"
      },
      "outputs": [],
      "source": [
        "#convert bytes to unicode \n",
        "data = all_data.decode(\"utf-8\")\n",
        "#we have to do this because the all_data typy is byte encoded to \"utf-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnRLufkv5QSk",
        "outputId": "9491d5f8-5a7f-4fc1-df16-8d8b5d4bfd8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "München Ü\n",
            "b'\\xc3\\xbc'\n",
            "b'M\\xc3\\xbcnchen \\xc3\\x9c'\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "#An exampe of how utf-8 encoding works:\n",
        "array = \"München Ü\"\n",
        "byte_array = array.encode(\"utf-8\")\n",
        "char_to_find = \"ü\".encode(\"utf-8\")  # encoded representation of \"ü\" in utf-8\n",
        "index_of_char = byte_array.index(char_to_find)\n",
        "print(array)\n",
        "print(char_to_find)\n",
        "print(byte_array)\n",
        "print(index_of_char)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEpVFE8vRQmM"
      },
      "outputs": [],
      "source": [
        "# Check if the  string contains illegal characters: -~ -ÿĀ-῿Ⰰ-퟿豈-﷏ﷰ-�\n",
        "def good_string(string_):\n",
        "    if re.search(r\"[^\\u0020-\\u007E\\u00A0-\\u00FF\\u0100-\\u1FFF\\u2C00-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD]\", string_):\n",
        "        return False\n",
        "\n",
        "    if (string_ == None): return False\n",
        "    # Check if the string is properly encodable in UTF-8\n",
        "    try:\n",
        "        string_.encode(\"utf-8\")\n",
        "    except UnicodeEncodeError:\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tGTyNmIRTYV"
      },
      "outputs": [],
      "source": [
        "#check if the string is actually a number\n",
        "def is_number(string_):\n",
        "    try:\n",
        "        float(string_)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4xShLm3RVrM"
      },
      "outputs": [],
      "source": [
        "#does the string contain just ASCII charachters? For example, Ü,Ö,Ä are not ASCII\n",
        "def has_only_ascii(string_):\n",
        "    return all(char.isascii() for char in string_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHo8GyFgBs2x",
        "outputId": "28ec6c1a-a0e1-4df7-f7fd-3d49dff084b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True False False False\n"
          ]
        }
      ],
      "source": [
        "#Example of ASCII vs non-ASCII characters recognized inside strings by has_only_ascii() function\n",
        "print(has_only_ascii(\"Munich\"), has_only_ascii(\"München\"), has_only_ascii(\"Мюншен\"), has_only_ascii(\"مونیخ\")  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW4Yc10WRaNC"
      },
      "outputs": [],
      "source": [
        "#define an empty dataset/DataFrame\n",
        "df_clicks = pd.DataFrame(columns=['from','clicks','to']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAY7Qld0hKY"
      },
      "source": [
        "##Data preprocessing\n",
        "<a name=\"for_loop\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PZNU5s4hQFM3"
      },
      "outputs": [],
      "source": [
        "for line in data.split(\"\\n\"):\n",
        "\n",
        "    columns = line.split(\"\\t\")\n",
        "\n",
        "    # Skip the header row and the row containing non-keyword strings\n",
        "    if columns[0] == \"source\" or len(columns)<2 \\\n",
        "    or columns[0] == \"other-empty\" \\\n",
        "    or \"other-search\" in columns[0] or \"other-internal\" in columns[0] or \"other-external\" in columns[0]\\\n",
        "    or \"other-other\" in columns[0] or \"other-other\" in columns[2]\\\n",
        "    or \"other-empty\" in columns[1]\\\n",
        "    or \"other-search\" in columns[1] or \"other-internal\" in columns[1] or \"other-external\" in columns[1]\\\n",
        "    or \"other-empty\" in columns[2]\\\n",
        "    or \"other-search\" in columns[2] or  \"other-internal\" in columns[2] or \"other-external\" in columns[2]\\\n",
        "    or not is_number(columns[3]) or not has_only_ascii(columns[0])or not has_only_ascii(columns[1]):\n",
        "        continue\n",
        "\n",
        "    #replace special characters in URLs with xx% charachters e.g.: \"+\" -> \"%2B\"   \n",
        "    source = quote(columns[0]) #from\n",
        "    target = quote(columns[1]) #to\n",
        "    clicks = columns[3] #number of clicks\n",
        "\n",
        "    #if the everyithing is ok; not bad charachter is in the strings, make a \n",
        "    #dictionary out of them and add the to them and add to the DataFrame\n",
        "    if good_string(source) and good_string(target):\n",
        "        df_clicks = df_clicks.append({\"from\":source, \"clicks\":clicks, \"to\":target},ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vbYWS2t6Da_"
      },
      "source": [
        "###How many rows/recods do we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K6eITOIj6I8R",
        "outputId": "97435609-3e5b-47e5-9522-5f586425857b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of records: 64164\n"
          ]
        }
      ],
      "source": [
        "print(\"number of records:\",len(df_clicks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz9VVZ5w57W_"
      },
      "source": [
        "##Replacing non-informative strings with null and then removing nulles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lUp1NddpT7at"
      },
      "outputs": [],
      "source": [
        "#remove all rows containing empty strings of null values\n",
        "df_clicks = df_clicks.replace(\"\", np.nan)\n",
        "df_clicks.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4fwNlX-6Ur2"
      },
      "source": [
        "###How many recodes remained after removing null values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8IL13UZ9FwKF",
        "outputId": "3789468c-52a8-4ae8-a95f-8bdab0793708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of records: 64164\n"
          ]
        }
      ],
      "source": [
        "#see how many rows remained:\n",
        "print(\"number of records:\",len(df_clicks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkOKJmnFAjqM"
      },
      "source": [
        "##Merge the data of all months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vJGxseIb0MEW"
      },
      "outputs": [],
      "source": [
        "#summ all clicks with the identical \"from\" and \"to\" fileds. These identical rows \n",
        "#come from different months but the same source and target pages.\n",
        "df_clicks = df_clicks.groupby([\"from\",\"to\"]).sum() \n",
        "df_clicks.reset_index(inplace = True) #reorder the indecies after applying sum\n",
        "df_clicks = df_clicks[[\"from\",\"clicks\",\"to\"]] #reorder the columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kuxmrr5A1EB"
      },
      "source": [
        "###How many recodes remained after merging months?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nt4lAZiNA5qw",
        "outputId": "be249857-b173-451f-c91e-799f4b0e1185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of records: 17834\n"
          ]
        }
      ],
      "source": [
        "#see how many rows remained:\n",
        "print(\"number of records:\",len(df_clicks))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQrH4hx3BOZl"
      },
      "source": [
        "##Saving the results not to always need loading and preprocessing the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loVSa-bx_Moq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa170e22-85f1-4fb4-9491-da9971988da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Saving the data to Google Drive\n",
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive\", force_remount=True)\n",
        "#df_clicks.to_csv('/content/drive/MyDrive/Datasets/wikipedia_clicks.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKHvMew_C_aJ"
      },
      "source": [
        "<a name=\"cell-id\"></a>\n",
        "##If the data is already preprocessed and saved on Google Drive, just load it from the CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JxrUgOqDL8c"
      },
      "outputs": [],
      "source": [
        "#df_clicks = pd.read_csv('/content/drive/MyDrive/Datasets/wikipedia_clicks.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbNfgNCtcJdS"
      },
      "source": [
        "###Definition of child of a page: \n",
        "A page page_child is the child of the page_parent, if in the df_clicks dataset, there is any row in which page_parent is in the \"from\" and the page_child is in the \"to\" field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtpqcAfRL2p5"
      },
      "source": [
        "###Rerieve page: <a name=\"function\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJUGphJSwhWS"
      },
      "outputs": [],
      "source": [
        "df_clicks.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCfYRfsCpa49"
      },
      "outputs": [],
      "source": [
        "#from wikipedia.wikipedia import random\n",
        "#Get the page related to the keyword/query\n",
        "def get_page(query): \n",
        "  #Search for matching pages to the query/keyword\n",
        "  results = wikipedia.search(query)\n",
        "\n",
        "  if len(results) == 0:#if no page is found, as Wikipedia to suggest a page\n",
        "    suggestion = wikipedia.suggest(query)\n",
        "    return wikipedia.page(suggestion) #return the suggested page\n",
        "\n",
        "  if len(results) == 1:#if just one keword is found\n",
        "    try:#try to find the related page and return it\n",
        "      return wikipedia.page(results)\n",
        "    except:#if actually no related page is there on Wikipedia, return Null\n",
        "      return None\n",
        "\n",
        "  #we are going to find the page with the maximum number of children pages\n",
        "  max_child = -1 #initial value for the maximum number of pages\n",
        "  max_page = None\n",
        "\n",
        "  for i in range(len(results)): #for the number of found keywords,\n",
        "    #take keywords one by one\n",
        "    res = results[i] \n",
        "    #how many children does the page have?/how many times people have gone from this page to anothe one?\n",
        "    n_child = len(df_clicks[df_clicks[\"from\"].str.contains(query,case=False)])\n",
        "    if n_child > max_child: #if the number of the children is bigger than the number of the current maximum number of children\n",
        "      max_child = n_child #set the new number of chilred as the maximum and \n",
        "      try: #try to find the page \n",
        "        max_page = wikipedia.page(res) \n",
        "      except:\n",
        "        try:#try to find the related page and return it\n",
        "          max_page =  wikipedia.page(res)\n",
        "        #except:#if actually no related page is there on Wikipedia, return Null\n",
        "        except Exception as e:\n",
        "          print(f'caught {type(e)}: e')\n",
        "          print(e)\n",
        "          return None\n",
        "\n",
        "  return max_page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxIhRJr169K-"
      },
      "source": [
        "##Class/Type/Abstraction/Defenition of an Article:\n",
        "###Each article is packed inside a class\n",
        "<a name=\"class\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Suw_UShGfW"
      },
      "outputs": [],
      "source": [
        "#each artice and it's parameters is considered as an objet of the type class \"article\"\n",
        "class article:\n",
        "  #constructor function; initializes all attributes other than the ones which require\n",
        "  #web requests using Wikipedia API.\n",
        "  def __init__(self,keyword, parent = [], children = []):\n",
        "    self.keyword = keyword #wikipedia keyword\n",
        "    self.parent = parent #from which page we people come to the page having the \"keyword\"?\n",
        "    self.children = children #to which page people gone from the page having the \"keyword\"?\n",
        "    self.fill_article_info() #find URL and abstract of the page containing the \"keyword\"\n",
        "  \n",
        "  def fill_article_info(self):    \n",
        "    #retrieves page title and URL and initializes the respective attributes\n",
        "    page_title = get_page(self.keyword) #find the page which moslty matched the keyword\n",
        "    self.page = page_title #get the title of the page\n",
        "    self.article_url = self.page.url if self.page != None else \"\" #get the urls of the page\n",
        "    self.summary = self.page.summary if self.page != None else \"\" #get the urls of the page\n",
        "    print(self.article_url) #print out the URL \n",
        "    print(self.summary) #print out the summary\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6XHhBiYMy55"
      },
      "source": [
        "###Article Tree Class\n",
        "<a name=\"class_inheritance\"></a>\n",
        "<a name=\"while_loop\"></a>\n",
        "<a name=\"recursive_function\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "775MHOWUkzEo"
      },
      "source": [
        "###For each search with specific depth and width, a tree data structure made of article classes is built. Instead of calling a search function recursively and saving the resulting keywords and URLs in separate lists, each found article is packed in an article class. Consecutive linked articles based on the click or search data are stored in a tree class which also preserves the relations among articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amwnK3XKlhed"
      },
      "outputs": [],
      "source": [
        "class article_tree:\n",
        "\n",
        "  def __init__(self, root_keyword, n_depth, n_branches, all_children=None):\n",
        "    #constructor of the class\n",
        "    self.root_keyword = root_keyword\n",
        "    self.n_depth = n_depth #how many articles/links/steps do we want to go from here?\n",
        "    self.n_branches = n_branches #how many links on each page are considered for search?\n",
        "    self.all_children = {root_keyword} #set of all children; used to avoid repeatation\n",
        "    self.build_tree() #build the tree of artciles\n",
        "\n",
        "\n",
        "  def get_children(self, keyword,n_branches):\n",
        "    #finds out all children of the article matching the \"keyword\"; depth: 1, n_branches barnches\n",
        "    children = df_clicks[df_clicks[\"from\"].str.contains(keyword,case=False)].sort_values(\"clicks\",ascending=False).iloc[0:n_branches].to.values\n",
        "    children = list(children) #list of most frequenlty clicked articles from the page matching \"keyword\"\n",
        "\n",
        "    n_child = len(children) #number of the children of the current page; it can be less than n_branches\n",
        "\n",
        "    for ch in self.all_children:\n",
        "      self.all_children.add(ch)\n",
        "    #in case the number of children found based on the click data is less than n_branches\n",
        "    if n_child < n_branches :\n",
        "      results = wikipedia.search(keyword) #search for similar pages matching the keyword\n",
        "      i = n_child #child count\n",
        "      k = 0 #index of results[]\n",
        "      #go through the search results and add them to the children untill the number of children is equal to n_branches\n",
        "      while i < n_branches and k<len(results): \n",
        "        if results[k] not in self.all_children:        \n",
        "          self.all_children.add(results[k])\n",
        "          children.append(results[k]) \n",
        "          i += 1\n",
        "        k += 1\n",
        "    return children\n",
        "\n",
        "  def build_children(self, root_keyword, n_depth, n_branches ): \n",
        "    #recursively builds a tree of artciles with the depth of n_depth \n",
        "    root = article(root_keyword) #the root/starting article\n",
        "    if n_depth == 1: #return condition of recursion\n",
        "      return root\n",
        "    \n",
        "    children_names = self.get_children(root_keyword,n_branches)#children of the root\n",
        "    \n",
        "    children = [] \n",
        "    for child_name in children_names: #make artcile class objects out of each child\n",
        "      \n",
        "      art_ = self.build_children(child_name, n_depth-1, n_branches)\n",
        "\n",
        "      children.append(art_) #add the child to the list of the children\n",
        "    root.children = children\n",
        "    \n",
        "    #return the root which alread has children and grand children\n",
        "    return root\n",
        "\n",
        "  def build_tree(self):   #start the recursion\n",
        "    self.root = self.build_children(self.root_keyword, self.n_depth, self.n_branches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXF7uwCQ8qMq"
      },
      "outputs": [],
      "source": [
        "df_clicks.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdcsKkbyGiJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9950b63-5d9d-4b46-d37b-a093301a9431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
            "Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
            "Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n",
            "The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
            "Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
            "Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n",
            "The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Multi-agent_reinforcement_learning\n",
            "Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment. Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.\n",
            "Multi-agent reinforcement learning is closely related to game theory and especially repeated games, as well as multi-agent systems. Its study combines the pursuit of finding ideal algorithms that maximize rewards with a more sociological set of concepts. While research in single-agent reinforcement learning is concerned with finding the algorithm that gets the biggest number of points for one agent, research in multi-agent reinforcement learning evaluates and quantifies social metrics, such as cooperation, reciprocity, equity, social influence, language and discrimination.\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Q-learning\n",
            "Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations.\n",
            "For any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. \"Q\" refers to the function that the algorithm computes – the expected rewards for an action taken in a given state.\n",
            "\n",
            "\n",
            "caught <class 'wikipedia.exceptions.PageError'>: e\n",
            "Page id \"machine ;earning\" does not match any pages. Try another id!\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Deep_reinforcement_learning\n",
            "Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Deep_learning\n",
            "For the \"South Park\" episode, see \"Deep Learning (South Park)\".\n",
            "Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability.\n",
            "https://en.wikipedia.org/wiki/Adversarial_machine_learning\n",
            "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.To understand, note that most machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\n",
            "Some of the most common threat models in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Machine_learning_in_video_games\n",
            "In video games, various artificial intelligence techniques have been used in a variety of ways, ranging from non-player character (NPC) control to procedural content generation (PCG). Machine learning is a subset of artificial intelligence that focuses on using algorithms and statistical models to make machines act without specific programming. This is in sharp contrast to traditional methods of artificial intelligence such as search trees and expert systems.\n",
            "Information on machine learning techniques in the field of games is mostly known to public through research projects as most gaming companies choose not to publish specific information about their intellectual property. The most publicly known application of machine learning in games is likely the use of deep learning agents that compete with professional human players in complex strategy games. There has been a significant application of machine learning on games such as Atari/ALE, Doom, Minecraft, StarCraft, and car racing. Other games that did not originally exists as video games, such as chess and Go have also been affected by the machine learning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-08587221d374>:25: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  n_child = len(df_clicks[df_clicks[\"from\"].str.contains(query,case=False)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)\n",
            "In reinforcement learning (RL), a model-free algorithm (as opposed to a model-based one) is an algorithm which does not use the transition probability distribution (and the reward function) associated with the Markov decision process (MDP), which, in RL, represents the problem to be solved. The transition probability distribution (or transition model) and the reward function are often collectively called the \"model\" of the environment (or MDP), hence the name \"model-free\". A model-free RL algorithm can be thought of as an \"explicit\" trial-and-error algorithm. An example of a model-free algorithm is Q-learning.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-3a120f271abe>:14: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  children = df_clicks[df_clicks[\"from\"].str.contains(keyword,case=False)].sort_values(\"clicks\",ascending=False).iloc[0:n_branches].to.values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://en.wikipedia.org/wiki/Temporal_difference_learning\n",
            "Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of bootstrapping, as illustrated with the following example:\n",
            "\n",
            "Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday – and thus be able to change, say, Saturday's model before Saturday arrives.\n",
            "Temporal difference methods are related to the temporal difference model of animal learning.\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Proximal_Policy_Optimization\n",
            "Proximal Policy Optimization (PPO) is a family of model-free reinforcement learning algorithms developed at OpenAI in 2017. PPO algorithms are policy gradient methods, which means that they search the space of policies rather than assigning values to state-action pairs.\n",
            "PPO algorithms have some of the benefits of trust region policy optimization (TRPO) algorithms, but they are simpler to implement, more general, and have better sample complexity. It is done by using a different objective function.\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Multi-armed_bandit\n",
            "In probability theory and machine learning, the multi-armed bandit problem (sometimes called the K- or N-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice. This is a classic reinforcement learning problem that exemplifies the exploration–exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling.\n",
            "In the problem, each machine provides a random reward from a probability distribution specific to that machine, that is not known a-priori. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. The crucial tradeoff the gambler faces at each trial is between \"exploitation\" of the machine that has the highest expected payoff and \"exploration\" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in machine learning. In practice, multi-armed bandits have been used to model problems such as managing research projects in a large organization, like a science foundation or a pharmaceutical company. In early versions of the problem, the gambler begins with no initial knowledge about the machines.\n",
            "Herbert Robbins in 1952, realizing the importance of the problem, constructed convergent population selection strategies in \"some aspects of the sequential design of experiments\". A theorem, the Gittins index, first published by John C. Gittins, gives an optimal policy for maximizing the expected discounted reward.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "keyword = \"reinforcement learning\" #look for recommended articles to learn the \"keyword\"\n",
        "tree_1 = article_tree(keyword,3,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYf7226sCUce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OUVOCRcQXkm0w87WCCPEUFC2JXDZsPeC",
      "authorship_tag": "ABX9TyMHxbT0ZPbge7wzsjfA0LeK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}